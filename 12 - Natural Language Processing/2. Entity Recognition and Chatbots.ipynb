{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Creating Chatbots and recognizing entities</div>\n",
    "\n",
    "In this exercice, we will apply the example of text classification studied previously to the creation of chatbots.\n",
    "\n",
    "Building an agent capable of handling conversations is usually performed through the following steps:\n",
    "1. We apply text classification techniques to the utterance typed by the user, in order to detect the **intent** of the user (e.g. \"applying for a loan\", \"purchasing a product from a store\", \"asking about the weather\", etc)\n",
    "2. In addition to detecting intent, we extract **Named Entities** from the utterance (e.g. location names, currency amounts, etc...)\n",
    "3. We use the extracted intents and entities to decide how the chatbot should reply\n",
    "\n",
    "In this exercise, we will use the open source chatbot framework **Rasa** to build the chatbot. It includes the package *Rasa NLU* that performs NLU tasks such as entity extraction and intent classification, as well as *Rasa Core*, that manages the conversational aspects.\n",
    "\n",
    "# 1. Using Rasa\n",
    "\n",
    "Rasa runs code asynchronously. The following cell configures the notebook for Rasa use. See [the documentation of Rasa](https://rasa.com/docs/rasa/api/jupyter-notebooks/) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Rasa will throw plenty of warnings during execution. This line gets rid of them, for lisibility. \n",
    "# Just comment it to keep the warnings if desired\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rasa chatbot project folders follow a precise structure to function. Important files to consider are:\n",
    "1. A **config.yml** file defining the language of the chatbot, the methods used for intent classification, etc...\n",
    "2. A **domain.yml** file, defining the intents and entities covered by the chatbot, as well as the utterances and actions it will use. Generally, most of the engineering of chatbots goes towards this file.\n",
    "3. A **nlu** file (Rasa uses json or md files), containing the training samples for intent classification and entity extraction.\n",
    "4. A **stories** file, contaning the different scenariis of interaction between the user and the bot. This file defines how to associate bot actions and intents/entities.\n",
    "\n",
    "When starting a new project, Rasa provides a useful function that prepares the folder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa.cli.scaffold import create_initial_project\n",
    "\n",
    "project = \"my_new_chatbot_project\"\n",
    "create_initial_project(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building our first chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/1_simple_clockbot/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first chatbot is meant to be a simple clockbot (a chatbot that gives the time). Let's go through the structure of the files to perform this task.\n",
    "\n",
    "## 2.a. config file\n",
    "\n",
    "```\n",
    "# Configuration for Rasa NLU.\n",
    "# https://rasa.com/docs/rasa/nlu/components/\n",
    "language: fr\n",
    "pipeline: supervised_embeddings\n",
    "```\n",
    "\n",
    "The chatbot is meant to be in french. The pipeline we use is the default Rasa one, **supervised_embeddings**.\n",
    "\n",
    "In the text classification examples we saw in the previous exercise, we learnt individual vector representations of every word in our corpus, and used these as features for classification. We saw that it causes issues with out-of-vocabulary (OOV) words, and with model storage. The main modification to embeddings proposed by Rasa is to learn intent representations as ell as words'. This means that any input sequence can be judged in terms of similarity with an intent, making it easier and less costly to perform classification. You can read more about this [here](https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8)\n",
    "\n",
    "## 2.b. domain file\n",
    "\n",
    "```\n",
    "intents:\n",
    "  - greet\n",
    "  - ask_time\n",
    "\n",
    "actions:\n",
    "- utter_greet\n",
    "- utter_time\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "  - text: \"Bien le bonjour !\"\n",
    "  utter_time:\n",
    "  - text: \"Il est 10h\"\n",
    "```\n",
    "\n",
    "As introduced earlier, this first chatbot will:\n",
    "- cover 2 intents (greeting and asking what time it is)\n",
    "- perform only two possible actions (greet the user back, or give the time)\n",
    "\n",
    "Both actions performed by the chatbot in this example are single utterances, defined within the file itself. It would be possible to declare more complex actions, ran separately in an \"action server\" (see [the documentation](https://rasa.com/docs/rasa/core/actions/#actions)). As the objective of this exercise is not to define an action server, but rather to explore machine learning capabilities of chatbots, we propose to simplify the problem by creating a dumb clockbot stuck at 10 o'clock.\n",
    "\n",
    "## 2.c. stories file\n",
    "\n",
    "```\n",
    "## greet path\n",
    "* greet\n",
    "  - utter_greet\n",
    "\n",
    "## time_path_polite\n",
    "* greet\n",
    "  - utter_greet\n",
    "* ask_time\n",
    "  - utter_time\n",
    "  - action_restart\n",
    "```\n",
    "\n",
    "We define only two main scenarii for interaction with this chatbot :\n",
    "- the user greets the chatbot, but interacts no further\n",
    "- the user greets the chatbot, then asks what time it is (the action_restart at the end of this path reinitializes the bot once it has given the time)\n",
    "\n",
    "Note that we have not defined a scenario where the user would ask the time without greeting the robot first (we will cover this in the next exercise).\n",
    "\n",
    "## 2.d. nlu file\n",
    "\n",
    "```\n",
    "## intent:greet\n",
    "- salut\n",
    "- hello\n",
    "- yo\n",
    "- slt\n",
    "- bonjour\n",
    "- bonsoir\n",
    "\n",
    "## intent:ask_time\n",
    "- il est quelle heure ?\n",
    "- quelle heure est-il\n",
    "- t'as l'heure stp ?\n",
    "```\n",
    "\n",
    "This file contains examples for all the intents that we have defined for the chatbot. Note that we included some examples containing abbreviations, mispellings, etc... Even if NLU models are designed to be robust to misspellings when trained on correctly spelled utterances, it is a good practice to include examples \"from the real world\" for each intent, and to include them as such (without correcting misspellings).\n",
    "\n",
    "## 2.e. Training the chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mCore stories/configuration did not change. No need to retrain Core model.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████████████████████████████████████████████████| 300/300 [00:24<00:00, 12.40it/s, loss=0.288, acc=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mNLU model training completed.\u001b[0m\n",
      "\u001b[92mYour Rasa model is trained and saved at 'C:\\Users\\DURANTGA\\Documents\\Dev\\Cours NLP\\MLclass\\12 - Natural Language Processing\\chatbot_projects\\1_simple_clockbot\\models\\20191126-150509.tar.gz'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import rasa\n",
    "\n",
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.f. Using the chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpcnwnugkz\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpcnwnugkz\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your bot is ready to talk! Type your messages here or send '/stop'.\n",
      "/stop\n"
     ]
    }
   ],
   "source": [
    "from rasa.jupyter import chat\n",
    "chat(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.g. Checking the behaviour of the bot classifier\n",
    "\n",
    "In this section, we will cofirm the resilience of the bot to mistakes during typing.\n",
    "We first load the model in to an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmppae749wu\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmppae749wu\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    }
   ],
   "source": [
    "from rasa.core.agent import Agent\n",
    "agent=Agent.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following code, we can then parse various messages and see how the nlu classifier ranks them.\n",
    "We can check that :\n",
    "- standard messages for the intents (e.g. \"salut\") get classified correctly, with very high confidence levels\n",
    "- messages that vary quite extensively from the canonial question used during training can sill get classified correctly, but with lower confidence levels (e.g. using sms-style \"kel h?\")\n",
    "- if the messages differ too much from the samples used during training, the confidence level will decrease dramatically. \n",
    "\n",
    "**WARNING** : This example can be biased, as we are using only two intents (classes) with very different syntax (greetings are mostly single words, while more complex sentences are used to ask for time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'greet', 'confidence': 0.9999988079071045},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'greet', 'confidence': 0.9999988079071045},\n",
       "  {'name': 'ask_time', 'confidence': 1.1354217122061527e-06}],\n",
       " 'text': 'salut'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"salut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.9470230340957642},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.9470230340957642},\n",
       "  {'name': 'greet', 'confidence': 0.05297698825597763}],\n",
       " 'text': 'kel h'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.6792893409729004},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.6792893409729004},\n",
       "  {'name': 'greet', 'confidence': 0.3207106590270996}],\n",
       " 'text': 'compte'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"compte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Adding different paths\n",
    "\n",
    "In the previous example, we did not consider the case when a user would ask what time it is without greeting the bot first. In this example, we propose to adapt the scenarii of user interaction to handle those cases.\n",
    "\n",
    "The main modification is located in the **stories** file, where the following story is added :\n",
    "```\n",
    "## time_path_impolite\n",
    "* ask_time\n",
    "  - utter_impolite\n",
    "```\n",
    "\n",
    "Note that an action (utterance) has been added to handle the situation where the bot gets offended because the user did not greet it. This is also reflected in the **domain.yml** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/2_obnoxious_clockbot/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our bot and run it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mNothing changed. You can use the old model stored at 'C:\\Users\\DURANTGA\\Documents\\Dev\\Cours NLP\\MLclass\\12 - Natural Language Processing\\chatbot_projects\\2_obnoxious_clockbot\\models\\20191126-143830.tar.gz'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells below run the polite scenario (greeting + asking for time) and the impolite one (asking for time directly).\n",
    "We confirm here that the bot behaves differently in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpmeqrup8k\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpmeqrup8k\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'default', 'text': 'Bien le bonjour !'}]\n",
      "[{'recipient_id': 'default', 'text': 'Il est 10h'}]\n"
     ]
    }
   ],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"slt\"))\n",
    "print(await agent.handle_text(\"il est kel h ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmp0evn58d4\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmp0evn58d4\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'default', 'text': 'Tu pourrais au moins me dire bonjour !'}]\n"
     ]
    }
   ],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"il est kel h ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding entities\n",
    "\n",
    "In this example, we will add **Named Entity extraction** to the bot. \n",
    "\n",
    "Named Entities can be viewed as instances of linguistic classes. For instance, there is an inifinite number of ways through which one might refer to a Location. Some examples could be \"Paris\", \"my house\", \"Japan\", \"the restaurant I like on Main Street\", etc... From a Named Entity Recognition (NER) perspective, all these examples would be specific **values** (or synonyms) refering to the **entity** \"location\".\n",
    "\n",
    "## 4.a How are entities extracted by Rasa ?\n",
    "\n",
    "### Symbolic approach\n",
    "\n",
    "The simplest way of approaching Named Entity Extraction is to define all synonyms of an entity manually. In this case, the user would type all possible values of an entity manually. The extraction of these entities would then be performed by matching words in an utterance with the list of synonyms.\n",
    "\n",
    "There are some cases where this mode of definition is appropriate. For instance, to define the entity \"*means of payment*\", there are only limited options (*cash*,*cheque*,*credit card*,*debit card*...). This approach, however, has two main issues :\n",
    "- the maintenance of synonym lists can quickly become impossible (for instance when defining entities corresponding to person or loction names)\n",
    "- this approach is not resilient to spelling variants (e.g. typing \"crdt card\" instead of \"credit card\"). \n",
    "\n",
    "The second issue can be mitigated by introducing \"flexible\" or \"fuzzy\" matching for entities (i.e. considering that a word that differs by only one character from one of the synonyms is still a match). However, in many cases, this is still imperfect.\n",
    "\n",
    "### Statistical approach\n",
    "\n",
    "Rasa relies on **extractors** for entities. For instance, the default CRFExtractor uses Conditional Random Fields fitted on word embeddings to predict whether a given word is an entity. In practice, that means that entities are detected using their representation, as well as the representations of the words in their surrounding.  \n",
    "\n",
    "In our case, using this approach means that giving enough examples in the form \"Quelle heure est-il à Paris\", \"quelle heure est-il à Londres\", etc... will enable the extractor to understand that in the pattern \"quelle heure est-il à ....\", the word following \"à\" is a \"location\".\n",
    "\n",
    "## 4.b. Defining location entities for our chatbot\n",
    "\n",
    "We propose to add the capacity to ask what the time is at a given location to our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project location\n",
    "project = \"chatbot_projects/3_obnoxious_clockbot_with_entities/\"\n",
    "\n",
    "#path to config, domain, nlu and stories files\n",
    "config = project+\"config.yml\"\n",
    "domain = project+\"domain.yml\"\n",
    "training_files = project+\"data/\"\n",
    "\n",
    "#where to store the models\n",
    "output = project+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial modification is located in the **domain** file, where the entity type \"*location*\" is defined. We also use the value of this entity in the \"*utter_time*\" utterance.\n",
    "```\n",
    "entities:\n",
    "  - location\n",
    "\n",
    "[...]\n",
    "\n",
    "  utter_time:\n",
    "  - text: \"Il est 10h à {location}\"\n",
    "```\n",
    "\n",
    "The training data in the **nlu** file is also modified to reflect this change :\n",
    "\n",
    "```\n",
    "## intent:ask_time\n",
    "- il est quelle heure à [New York](location)\n",
    "- quelle heure est-il à [Paris](location)\n",
    "- quelle heure est-il à [Séoul](location)\n",
    "- t'as l'heure de [Tokyo](location) stp ?\n",
    "- heure de [Lyon](location)\n",
    "```\n",
    "\n",
    "In the above examples, the sentence ```Quelle heure est-il à [Lyon](location)``` should be read as \"*Quelle heure est-il à Lyon*\", but where Lyon is an instance of the \"location\" entity.\n",
    "More information about the format of training data can be found in [the documentation](https://rasa.com/docs/rasa/nlu/training-data-format/).\n",
    "\n",
    "## 4.c. Training and testing the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mCore stories/configuration did not change. No need to retrain Core model.\u001b[0m\n",
      "\u001b[94mTraining NLU model...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████████████████████████████████████████████████| 300/300 [00:22<00:00, 13.39it/s, loss=0.348, acc=1.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mNLU model training completed.\u001b[0m\n",
      "\u001b[92mYour Rasa model is trained and saved at 'C:\\Users\\DURANTGA\\Documents\\Dev\\Cours NLP\\MLclass\\12 - Natural Language Processing\\chatbot_projects\\3_obnoxious_clockbot_with_entities\\models\\20191126-170742.tar.gz'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#training the model and saving the path where the model is stored\n",
    "model_path = rasa.train(domain, config, [training_files], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test the chatbot using a city name that was not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpw7nn86vl\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\DURANTGA\\AppData\\Local\\Temp\\1\\tmpw7nn86vl\\nlu\\component_6_EmbeddingIntentClassifier.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'recipient_id': 'default', 'text': 'Bien le bonjour !'}]\n",
      "[{'recipient_id': 'default', 'text': 'Il est 10h à Toulouse'}]\n"
     ]
    }
   ],
   "source": [
    "agent=Agent.load(model_path)\n",
    "print(await agent.handle_text(\"salut\"))\n",
    "print(await agent.handle_text(\"il est quelle heure à Toulouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an individual utterance, we can check that the model has learned to recognize entities based on the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.9998183846473694},\n",
       " 'entities': [{'start': 6,\n",
       "   'end': 14,\n",
       "   'value': 'Toulouse',\n",
       "   'entity': 'location',\n",
       "   'confidence': 0.7203844809327085,\n",
       "   'extractor': 'CRFEntityExtractor'}],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.9998183846473694},\n",
       "  {'name': 'greet', 'confidence': 0.00018160570471081883}],\n",
       " 'text': 'kel h Toulouse'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h Toulouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is one major limitation : the example above does not work anymore when the city name is not capitalized (toulouse instead of Toulouse). In fact, EntityExtractors traditionnally use word shapes (e.g. patterns of lowercase or uppercase letters) as predictors. In our training data, nearly all city names are capitalized. One way to correct this mistake would be to multiply examples with various capitalizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'name': 'ask_time', 'confidence': 0.9998183846473694},\n",
       " 'entities': [],\n",
       " 'intent_ranking': [{'name': 'ask_time', 'confidence': 0.9998183846473694},\n",
       "  {'name': 'greet', 'confidence': 0.00018160570471081883}],\n",
       " 'text': 'kel h toulouse'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.parse_message_using_nlu_interpreter(\"kel h toulouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

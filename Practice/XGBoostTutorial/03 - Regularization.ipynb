{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) and was written by Erwan Lecarpentier and Jonathan Sprauel.\n",
    "\n",
    "License: CC-BY-SA-NC.\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>\n",
    "\n",
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you learned the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we focused on **ensemble methods**.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by a few reminder on the Bias/Variance tradeoff :\n",
    "\n",
    "\n",
    "**Bias** is the mean error between our prediction and the correct value. A high bias means that our model makes large mistakes, even on the training dataset.\n",
    "\n",
    "**Variance** is the sensitivity of the prediction to small changes in the dataset. A high variance means that the model performs well on the training dataset, but poorly on the test dataset.\n",
    "\n",
    "Let's summarize that with a figure :\n",
    "\n",
    "<img src=\"img/1 xwtSpR_zg7j7zusa4IDHNQ.png\">\n",
    "\n",
    "In this figure, the center of the target is what we try to predict - as if we were trying to send an arrow in the bullseye zone.\n",
    "A model with a high bias cannot reach the center - the center of gravity of all its arrows is on the outer part of the target.\n",
    "A model with a low bias is globally centered on the target.\n",
    "\n",
    "A model with a high variance is not consistant - its arrows are largely spread.\n",
    "A model with low variance is very consistant - even when its arrows are not in the right place, they are in a limited zone.\n",
    "\n",
    "In a machine learning algorithm, it is a *tradeoff* between Bias and Variance : we want to minimize both (low bias, low variance), but usually when one decreases the other increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control overfitting in Gradient Boosted Trees, we can use 3 main leverages : tree structure, shrinkage and randomization.\n",
    "\n",
    "We already saw that the tree parameters, especially those related to tree depth such as max_depth and gamma, is a way to control the complexity of the model, and therefore the tradeoff bias/variance.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 1</b><br>\n",
    "      Complete the following code to compare the influence of the number of tree on both the bias and the variance. Completes the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def ground_truth(x):\n",
    "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
    "    return x * np.sin(x) + np.sin(2 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    \"\"\"generate training and testing data\"\"\"\n",
    "    np.random.seed(13)\n",
    "    x = np.random.uniform(0, 10, size=n_samples)\n",
    "    x.sort()\n",
    "    y = ground_truth(x) + 0.75 * np.random.normal(size=n_samples)\n",
    "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
    "    x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
    "    x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(200)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list, label='Test',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "ax.annotate('... bias', xy=(900, train_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.3), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... variance', xy=(900, test_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.4), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "One of the most usefull technic for regularization in Gradient Boosted Trees is the Shrinkage : the idea is to slow down the training by reducing the prediction of each tree by a scalar - the learning rate. In this way, the model must produce stronger concepts. \n",
    "\n",
    "A low learning rate impose a greater number of trees (n_estimators) to have a similar error during training - we are trading training time for some more precision.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 2</b><br>\n",
    "      Reuse and modify the previous code to compare two differente learning rates : 1.0 and 0.1. Concludes by completing the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "test_list_2 = []\n",
    "train_list_2 = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    est = ...\n",
    "    \n",
    "    est2 = ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list_2, label='Test 0.1',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list_2, label='Train 0.1', linewidth=2)\n",
    "\n",
    "ax.plot(estim_list, test_list, label='Test 1.0',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train 1.0', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Requires ... trees', xy=(200, train_list_2[199]), xycoords='data',\n",
    "            xytext=(300, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... test error', xy=(900, test_list_2[899]), xycoords='data',\n",
    "            xytext=(600, 0.5), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "As for many other algorithms, introducing randomness during training can lead to a greater precision at the end. There to main parameters that introduce randomness :\n",
    "* We can subsample the training data before growing each tree\n",
    "* We can subsample the features before choosing the best split node\n",
    "\n",
    "[XGBoost list of parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 3</b><br>\n",
    "      In XGBoost list of parameters, find the one that control randomness during training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A kaggle challenge : \n",
    "\n",
    "To apply what you have learned, we have selected a kaggle challenge that as been solved with XGBoost, a good preprocessing of the data and some fine parameters tuning.\n",
    "\n",
    "[Link to The Higgs-boson challenge (2014)](https://www.kaggle.com/c/higgs-boson/overview)\n",
    "\n",
    "Feel free to read the commentaries of the winning solutions - many of the top 30 solutions are using XGBoost in one way or another.\n",
    "\n",
    "You don't need to create an account on Kaggle, the data are in this github repo (higgs-boson.zip).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Final Exercice</b><br>\n",
    "      Use what you have learned to obtain the best score on this dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Sources :\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "This notebook is joint work by Erwan Lecarpentier, Luca Mossina and Emmanuel Rachelson.\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Support Vector Machines (practice session)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook you will find some data challenges to experiment with SVMs.\n",
    "\n",
    "Content:\n",
    "1. [Linear SVMs](#sec1)<br>\n",
    "1.1 [Let's play](#subsec11)<br>\n",
    "1.2 [Tune your linear SVM](#subsec12)\n",
    "2. [SVM with kernels](#sec2)<br>\n",
    "2.1 [Warm-up: Americans and Atheism  (OPTIONAL)](#subsec21)<br>\n",
    "2.2 [Back to basics (OPTIONAL)](#subsec22)<br>\n",
    "2.3 [Kernels](#subsec23)\n",
    "3. [Application: Binary classification](#sec3)\n",
    "4. [Application: Multi-label Classification (MLC)](#sec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a> 1. Linear SVMs\n",
    "\n",
    "The goal of this section is to explore the most basic form of SVM, namely the linear one. We are going to test this method on a toy data set and to play with the hyperparameters in order to get the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.1**<br>\n",
    "What is the objective of SVMs?<br>\n",
    "On which principle is it built? Give an **intuitive** geometrical interpretation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1\" class=\"collapse\">\n",
    "SVM is a supervised learning model used for **classification and regression** analysis.<br>\n",
    "It is built on the idea of **separating the data points by a hyperplane** maximizing the distance between the closest points and the hyperplane.\n",
    "This distance is called **the margin**.\n",
    "The points on each side of the hyperplane are then classified with different classes.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"subsec11\"></a> 1.1 Let's play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a toy data set $\\{ {x_i, y_i} \\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^2$ is a point and $y_i \\in [-1, 1]$ a class.<br>\n",
    "The code below allows you to load the data and to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size = (6,6)\n",
    "\n",
    "csv_file_name=\"data/data1.csv\"\n",
    "res = np.loadtxt(csv_file_name, delimiter=',')\n",
    "X = res[:,0:-1]\n",
    "y = res[:,-1].astype(int)\n",
    "Xblue = X[y==-1]\n",
    "Xred  = X[y==+1]\n",
    "fig = plt.figure(figsize=fig_size, dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(Xblue[:,0], Xblue[:,1], c='c', s=20)\n",
    "plt.scatter(Xred[:, 0], Xred[:, 1], c='r', s=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.1.1**<br>\n",
    "Can this data set be completely separated by a straight line?<br>\n",
    "What kind of technique could be used to train a classifier on this data set?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1-1-1\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1-1-1\" class=\"collapse\">\n",
    "This data set cannot be completely separated by a straight line as one cannot avoid points of the same class to be on both sides of the line.<br>\n",
    "To handle this, we can increase the tolerance, that is to say allowing a few points to lie on the wrong side of the hyperplane.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the SVM method to this dataset. To this purpose, we are going to use scikit-learn, a Python library providing most of the common Machine Learning off the shelf methods including SVM.  \n",
    "Their documentation page for this method can be found here: http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.1.2**<br>\n",
    "Use the class sklearn.svm.SVC provided in scikit-learn in order to train an SVM on the provided data.\n",
    "Try to display the separating hyperplane on this dataset. You can get inspiration from the SVM course's code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.1.3**<br>\n",
    "In \"Support Vector Machine\", what are the \"support vectors\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1-1-3\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1-1-3\" class=\"collapse\">\n",
    "The support vectors are data points that lie directly on the margin, or on the wrong side of the margin for their class. They _support_ the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"subsec12\"></a> 1.2 Tune your linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the parameters of the class sklearn.svm.SVC is the penalty parameter $C$ of the error term. It quantifies the missmatch-tolerance that is allowed when fitting an SVM model to the data. The smaller, the bigger is the tolerance. In our case, having such a tolerance is necessary because the data cannot be separated by a straight line, thus some point have to be on the wrong side of the separating hyperplan.\n",
    "\n",
    "$C$ is an hyperparameter, which means that it needs to be tuned for the algorithm to yield good results. This is a common concern in Machine Learning, as a result, techniques have been designed in order to select the best hyperparameters. The most common technique is probably the cross validation. Roughly it consists in:\n",
    "\n",
    "    0) dividing the data set into training and test sets;\n",
    "\n",
    "    1) selecting a value of the hyperparameters;\n",
    "\n",
    "    2) train the model with the training set;\n",
    "\n",
    "    3) test the model with the test set and compute a performance indicator;\n",
    "\n",
    "    4) go back to 1) with another value of the hyperparameters.\n",
    "\n",
    "When the procedure is finished, simply choose the hyperparameters yielding the best performance. We are now going to perform k-fold cross validation - which is a certain type of cross validation - on our problem, in order to identify the best value of $C$.\n",
    "For more details, see the wikipedia article: https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\n",
    "\n",
    "For more details on the type of indicators used in step 3), see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers<br>\n",
    "The accuracy, refered to as ACC in the article, may be a good start (and most scikit-learn classifiers implement it directly via the [`score` function](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.score))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.2.1**<br>\n",
    "Fill the code below in order to find the best value of the hyperparameter $C$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.001, 0.01, 0.1, 1.0, 10, 25, 30, 42, 100, 1000] # Tested values of C\n",
    "k = 10 # k-fold CV: number of subsets\n",
    "n = int(len(X)/k) # length of subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE - implement your own version of k-fold cross-validation.\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 1.2.2**<br>\n",
    "What is the best value of $C$ among the proposed ones?<br>\n",
    "What can you expect from the curve of the accuracy as a function of $C$ if you used a finer meshing for $C$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1-2-2\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1-2-2\" class=\"collapse\">\n",
    "The best values of $C$ are in $[10,25]$ they are equivalent.\n",
    "<br>\n",
    "With a finer meshing for $C$, we expect to see a piecewise constant function as we have a finite data set. Each jump of the function corresponds to the addition / removal of a new vector in the set of the misclassified points.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. SVMs with kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural application of SVM methods is that of binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"subsec21\"></a> 2.1 Warm-up: Americans and Atheism  (OPTIONAL)\n",
    "\n",
    "**SCOPE**: warm-up, revise and challenge yourself with a simple example.\n",
    "    - MAXIMUM TIME: 10 minutes, afterwards look at the proposed solution.\n",
    "\n",
    "Let us find out how americans tolerated atheists in 1976.\n",
    "The dataset is described as follows:\n",
    "\n",
    "```\n",
    "Dataset:  atheist.dat\n",
    "Source: E. Filsinger (1976). \"Tolerance of Non-Believers: A Cross-Tabular  and Log Linear Analysis of Some Religious Correlates,\" Review of Religious  Research, Vol.17, #3, pp.232-240\n",
    "Description: Church Attendance and Tolerance for Atheists for survey of 1221 people.\n",
    "\n",
    "Variables/Columns \n",
    "Church Attendance  8  /* 1=Never, 2=Yearly, 3=Monthly, 4=Weekly */\n",
    "Tolerance for Atheists  16 /* 1=Low, 2=High  */\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Retrieve and understand data\n",
    "\n",
    "The file is available online at: http://users.stat.ufl.edu/~winner/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.1.1.1**<br>\n",
    "Import the data as a pandas dataframe. No cheating, it can be done with a pandas function (one line).\n",
    "Your dataframe should have 1221 rows and 2 columns, which you'll name respectively 'attendance', 'tolerance'.\n",
    "<br><br>\n",
    "Plot the data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like the fancy figures we saw in class, does it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Run SVM. Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.1.2.1**<br>\n",
    "First of all, what can we learn having a strong look at the data and their description?\n",
    "<br>\n",
    "Do we have categorical, binary, continuous data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers2-1-2-1\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers2-1-2-1\" class=\"collapse\">\n",
    "The data take their values in a discrete space, hard to interprete numerically as well as practically. We could have imagined continuous data for the same descrition purpose.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are fundamental questions, which you'll ask yourselves for the rest of your (data science) life.\n",
    "Often, collegues or clients have no idea about these formalities; they have a problem and they ask you to provide a good enough solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the fun begin. We want to do SVM classification with scikit-learn, but we've already forgotten its api!  \n",
    "Thou shalt fear no more, the word of the Doc is with us:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm\n",
    "\n",
    "We are doing SVM classification, could it be: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC ?\n",
    "Let us copy-paste the example at the bottom of the page. Run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what are `X` and `y`? \n",
    "Let us take a moment to ponder on how to name things.  \n",
    "For sure, \"x\" and \"y\" mean something to all those with scientific background.  \n",
    "Often, however, when we do some ML modeling, we are having a data-dialogue with someone, ourselves included.  \n",
    "Do the favor to the \"you\" of the future and use a meaningful description for the variables; code is mostly meant to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attend = df['attendance'] # WARNING: you are passing a reference.\n",
    "toler  = df['tolerance']\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X = attend,\n",
    "        y = toler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something just went horribly wrong.  \n",
    "He's telling us something about ```\"Expected 2D array, got 1D array instead: [...]\"```  \n",
    "After some intense googling, stackoverflow.com gives us a hint:  \n",
    "https://stackoverflow.com/questions/38657138/scikits-learn-svm-1-dimensional-separating-hyperplane\n",
    "\n",
    "Let us add a dummy column of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['zeros'] = 0 # dummy zeros\n",
    "\n",
    "df  = df.sample(frac=1) # shuffle rows to avoid introducing bias\n",
    "print(df.iloc[0:5,])\n",
    "\n",
    "NTRAINS = 800 # out of 1221\n",
    "\n",
    "tr = df.iloc[0:NTRAINS,:] # training data\n",
    "te = df.iloc[NTRAINS: ,:] # testing data\n",
    "\n",
    "# assert: make sure now rows were lost or doubled\n",
    "assert tr.shape[0] + te.shape[0] == df.shape[0]\n",
    "\n",
    "features = ['attendance', 'zeros']\n",
    "attend = df[features]\n",
    "toler  = df['tolerance']\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(tol=1e-5) # linear\n",
    "# __uncomment__ if you want to try with nonlinear SVM:\n",
    "# clf = SVC(gamma='auto')\n",
    "\n",
    "clf.fit(X=attend[0:NTRAINS], y=toler[0:NTRAINS]) \n",
    "\n",
    "y_pred = clf.predict(attend[NTRAINS:])\n",
    "y_test = toler[NTRAINS:].tolist()\n",
    "\n",
    "errors = np.zeros(len(y_pred))\n",
    "\n",
    "for i in range(len(errors)):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        errors[i] = 1\n",
    "        \n",
    "er_rate = np.sum(errors) / len(errors)\n",
    "print(\"error rate:\", er_rate)\n",
    "print(\" --> about \", int(er_rate*100), \"% of the predictions were not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Campeones*, we made it, sklearn is no more mad at us.  \n",
    "Did we get anything interesting? Are we learning something at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--> Lesson learned: some data are more svm-able than others**\n",
    "\n",
    "Although SVM's are among the best tools in machine learning, we cannot expect it to work great in all cases.  \n",
    "Much more could be done, tuning parameters or changing kernels, but let's go on to more interesting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: beware of the Python   \n",
    "Let's look at what done just before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute and ignore the following two lines\n",
    "import copy            # IGNORE\n",
    "dF = copy.deepcopy(df) # IGNORE\n",
    "\n",
    "mangueira = dF['attendance'] # WARNING: you are passing a reference.\n",
    "portela   = dF['tolerance']\n",
    "\n",
    "# See how dangerous things can get:\n",
    "print(mangueira[0:2])\n",
    "mangueira[0:2] = 666, 666\n",
    "print(dF.iloc[0:2]) # original df is modified!!\n",
    "\n",
    "# Possible solution:\n",
    "import copy\n",
    "mangueira = copy.deepcopy(df['attendance'])\n",
    "# all data is hard copied in new struct: can be a problem, if data is big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move to more serious matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"subsec22\"></a> 2.2 Back to basics (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our previous little experiments, it is time to start gaining some insight into the use of kernels.  \n",
    "But what _is_ a kernel? Can we get an intuitive image of what these kernels are doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NPOINTS = 100\n",
    "COLORS  = ['pink', \"green\"]\n",
    "\n",
    "# Generate toy data\n",
    "x = np.random.rand(NPOINTS)\n",
    "y = np.ones(NPOINTS, dtype=int)\n",
    "for i in range(NPOINTS):\n",
    "    if x[i] < 0.0666 or x[i] > 0.666:\n",
    "        y[i] = -1\n",
    "        \n",
    "# Plot toy data: we have a one-dimensional problem\n",
    "plt.scatter(x[y==-1], np.zeros(len(x[y==-1])), color=COLORS[0])\n",
    "plt.scatter(x[y== 1], np.zeros(len(x[y==1])), color=COLORS[1])\n",
    "plt.xlabel('x');plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear separator, a point in this example, cannot be found to separate the data.  \n",
    "However we can easily see that the limit between the pink and the green regions are well defined.  \n",
    "How can we **transform** the data to make this linear separation possible? We can apply a function $f : x_i \\mapsto f(x_i)$ on each data point. Such function is called **feature** of the data.  \n",
    "Let us try with a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c = 1, 1, 1\n",
    "feature = a*x**2 + b*x + c\n",
    "\n",
    "plt.scatter(x[y==-1], feature[y==-1], color=COLORS[0])\n",
    "plt.scatter(x[y== 1], feature[y== 1], color=COLORS[1])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c = -8, 1, 2\n",
    "feature = a*x**2 + b*x + c\n",
    "\n",
    "plt.scatter(x[y==-1], feature[y==-1], color=COLORS[0])\n",
    "plt.scatter(x[y== 1], feature[y== 1], color=COLORS[1])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.2.1**<br>\n",
    "Copy the code above and play manually with the parameters in order to make the features separable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b, c = ?,?,?\n",
    "feature = a*x**2 + b*x + c\n",
    "\n",
    "colors = ['pink', \"green\"]\n",
    "\n",
    "plt.scatter(x[y==-1], feature[y==-1], color=COLORS[0])\n",
    "plt.scatter(x[y== 1], feature[y== 1], color=COLORS[1])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "height = 0.90\n",
    "plt.plot([0,1], [height, height], color='red')\n",
    "height = 0.95\n",
    "plt.plot([0,1], [height, height], color='blue')\n",
    "height = 1.00\n",
    "plt.plot([0,1], [height, height], color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouldn't be nice to have an algorithm doing that for us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"subsec23\"></a> 2.3 Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with the kernels in scikit-learn. As before, we are going to use the sklearn.svm.SVC class in order to fit SVMs to some data models. The 'kernel' attribute of the class allows you to use different kernels for the used models.\n",
    "\n",
    "Below are displayed several data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data(csv_file_name):\n",
    "    fig_size = (6, 6)\n",
    "    res = np.loadtxt(csv_file_name, delimiter=',')\n",
    "    X = res[:,0:-1]\n",
    "    y = res[:,-1].astype(int)\n",
    "    Xblue = X[y==-1]\n",
    "    Xred  = X[y==+1]\n",
    "    fig = plt.figure(figsize=fig_size, dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.scatter(Xblue[:,0], Xblue[:,1], c='c', s=20)\n",
    "    plt.scatter(Xred[:, 0], Xred[:, 1], c='r', s=20)\n",
    "\n",
    "plot_data(\"data/data2.csv\")\n",
    "plot_data(\"data/data3.csv\")\n",
    "plot_data(\"data/data4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.3.1**<br>\n",
    "Find a good kernel for each one of the data set above.<br><br>\n",
    "**Hint 1**: you can reuse the `plot_SVC` function provided in class that plots the decision frontier of the model and the data set $(X, y)$.<br><br>\n",
    "**Hint 2**: sometimes, the default kernels provided by scikit-learn are not enough. In this case, you can design a special kernel for your problem using the a callable function. See [here](http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for an example. Recall that $k(x,x')$ measures how close $x$ and $x'$ are in the kernel's projection space: keeping this in mind can help you shape a good projection first, and then deriving a good kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from myplot import plot_SVC\n",
    "\n",
    "def load_data(filename):\n",
    "    res = np.loadtxt(filename, delimiter=',')\n",
    "    X = res[:,0:-1]\n",
    "    y = res[:,-1].astype(int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 2.3.2**<br>\n",
    "Can you hint a method for robustly selecting the best kernel and or the hyperparameters of the kernel function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers2-3-2\" data-toggle=\"collapse\">**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers2-3-2\" class=\"collapse\">\n",
    "We could use cross validation to do this selection.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Application: Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to do binary classification on the Mushrooms data set loaded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/mushrooms.csv\")\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 classes, namely $y_i \\in \\{ e, p \\}$ and many weird values for the elements of $x_i$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 3.1**<br>\n",
    "Use LabelEncoder from sklearn.preprocessing in order to turn the elements of $x_i$ into numerical values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 3.2**<br>\n",
    "Use StandardScaler from sklearn.preprocessing in order to scale the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Question 3.3**<br>\n",
    "Now train an SVM classifier on the data. Try several different kernels and measure the accuracy of your predictor.<br>\n",
    "**Hint:** You can split your data set into a training and testing set using train_test_split from sklearn.model_selection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> 4. Application: Multi-label Classification (MLC)\n",
    "This is a hard one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see an application which is both harder and less common than binary classification, that of **multi-label classification** (MLC).  \n",
    "Given a list of possible labels, the problem consists in finding one **or more** labels associated to a data point.  \n",
    "For instance, imagine extracting the keywords from a newspaper article. Possibly many labels can be associated to a text; or for example classifing the elements composing an image\n",
    "\n",
    "Given a set of labels $\\mathcal{L} = \\{l_1, l_2, ..., l_k\\} \\in \\{0,1\\}^k$, we want to map elements of a feature space $\\mathcal{X}$ to a subset of $\\mathcal{L}$:  \n",
    "\n",
    "$$h : \\mathcal{X} \\longrightarrow \\mathcal{P}(\\mathcal{L})$$\n",
    "\n",
    "The two typical approaches for such problems are known as **Binary Relevance** (BR) and **Label Powerset** (LP).  \n",
    "\n",
    " - BR: each label in $\\mathcal{L}$ is a binary classification problem, $h_{i} : \\mathcal{X} \\longrightarrow l_{i}, l_{i} \\in \\{0,1\\}, i = 1, ..., |\\mathcal{L}|$.\n",
    "\n",
    " - LP: transforms a problem of MLC into one of multiclass classification, mapping elements $x \\in \\mathcal{X}$ directly to $s \\in \\mathcal{P}(\\mathcal{L})$.  \n",
    " This method becomes rapidly inapplicable as the number of $s$ grows exponentially.\n",
    " \n",
    " Many other variations exist, but for today we'll focus on BR, the most straightforward to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "<ul>\n",
    "<li> find a suitable package to load the file at `data/yeast.arff`.  <br>\n",
    "    Hint: [`scipy.io`](https://docs.scipy.org/doc/scipy/reference/io.html) and _read the doc_.\n",
    "<li> Store the data in a pandas dataframe.<br>\n",
    "    Hint: columns of classes will be encoded as 'utf-8', we need integers, look for 'str.decode('utf-8')'\n",
    "<li> check dataset: you should have 2417 samples $\\times$ 117 columns (103 features + 14 labels)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code8.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "<ul>\n",
    "<li> Manually, fit a SVM classifier for each label in the dataset\n",
    "<li> Apply a cross-validation of 60 âˆ• 40: 60% of datapoints to train the model, 40% to test it  <br>\n",
    "   Remember: it is good practice to **randomly shuffle** the data, in case the data are ordered w.r.t. some data-dependent criterion.\n",
    "<li> Report some performance measure\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code9.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably wondered _how_ to evaluate the performance of a MLC algorithm: well spotted, this is no easy task.   \n",
    "Evaluating classification performance can be challenging in binary classification, in multi-label classification this becomes harder and more ambiguous.\n",
    "\n",
    "The best starting point is the _Hamming_ loss. See for instance:\n",
    " - https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n",
    "\n",
    "This measures the average number of operations it would take to turn the vector of predictions $\\mathbf{y}$ into the correct one.\n",
    "$$ H_L  = 1 - \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\t\t\t\t\t\t\\left(\\frac{1}{L} \\sum_{k=1}^{L}\n",
    "\t\t\t\t\t\t\t{I}\\left(y_k^i = \\widehat{y}_k^i\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "<ul>\n",
    "<li> For 5 minutes: challenge yourself to speed-code a simple hamming loss function; it is useful, we promise.\n",
    "<li> **AFTER** 5 minutes, feel free to cheat and read the sklearn doc\n",
    "<li> from the Doc, look for other metrics and run them (accuracy, precision, recall, etc.). For reference, you can have a rapid look at the tables in:  <br>\n",
    "   page 349, https://link.springer.com/content/pdf/10.1007%2Fs10994-011-5256-5.pdf\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Hints\n",
    "def hamming_loss(predictions_list, observations_list):\n",
    "    assert len(predictions_list) == len(observations_list)\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Solution\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "print(\"Hamming Loss:\", hamming_loss(y_test[\"Class1\"], y_pred_01))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test[\"Class1\"], y_pred_01))\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important factors to keep in mind are the _Label Density_ and the _Label Cardinality_ .  \n",
    "\n",
    "Let us denote by $\\textbf{y}^i$ and $\\textbf{{y'}}^i$ respectively the observed  and predicted target vectors for the $i$-th row.\n",
    "\n",
    "$ LCard = \\frac{1}{N} \\sum_{i=1}^{N} \\left|\\textbf{y}^{i} \\right|$, tells us about the multi-labelness of data, yielding the average number of labels per target across the dataset.  \n",
    "\n",
    "$ LDens = \\frac{LCard}{|\\mathcal{L}|} \\times 100 = \\frac{1}{N |\\mathcal{L}|} \\sum_{i=1}^{N} \\left|\\textbf{y}^{i} \\right| \\times 100$, expresses what proportion of the available labels are, on average, associated to a data point $\\textbf{x}$.  \n",
    "We can use these to measure how sparse how our label vectors are.  \n",
    "Take a vector $y = [l_1, l_2, \\dots, l_{100}] $ in which only 2 of the labels take value 1: if we completely miss our prediction, and predict 2 other labels, 96 out of 100 labels will be \"correctly\" predicted as zero.  \n",
    "\n",
    "In MLC, it is good practice to use multiple metrics and compare them: the challenge is more complicated and focusing solely on one source of information could hide erros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "Measure the `LCard` and `LDens` of your predictions on  `yeast.arff`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_cardinality(dataset):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def label_density(dataset):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how powerful and versatile SVMs are. In MLC meta-algorithms, we have the concept of _base classifier_; this is the actual supervised learning tool that is giving us an estimate.  \n",
    "Often, the SVM is the best performing one, thus the default algorithm for many libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-challenges** :  \n",
    "\n",
    "<div class=\"alert alert-warning\">**Step 1**<br>\n",
    "Read the [documentation of sklearn on MLC](http://scikit-learn.org/stable/modules/multiclass.html) and experiment with different algorithms (no more manual coding).<br>\n",
    "We suggest you first try the **Classifiers Chains** (CC): see resources below.  <br>\n",
    "It is one of the reference meta-algorithms in MLC, and it gives its best when SVM are used as _base classifier_.\n",
    "</div>\n",
    "\n",
    "Strong with your intuitions on **metrics** (hamming loss, accuracy, etc.), **model validation** (CV, k-fold, etc.) and different **MLC architectures** (BR, CC), you are equiped with the right tools to show off your data-science awereness:\n",
    "\n",
    "<div class=\"alert alert-warning\">**Step 2**<br>\n",
    "Compare different methods on a **new dataset**:  http://mulan.sourceforge.net/datasets-mlc.html .  <br>\n",
    "From the list above, we suggest you try either on `emotions`, `scene` or `music`. They are simple enough to get you started rapidly and you can conveniently find them in the `data` directory. Inspect carefully the datasets and change accordingly the names of the classes and of the features.<br>\n",
    "Suggestion: aim for a results table where you can cross at least 2 MLC algos (CC, BR, etc.) with their respective Hamming Loss, accuracy etc.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">**Step 3**<br>\n",
    "Reflect on the weak points of SVM and the MLC approaches you have tested.  <br>\n",
    "Hint: what if we had 300 labels? How computationally costly would it be to fit 300 independent SVM? maybe with hundreds of features and tens of thousand of data points? <br>\n",
    "Hint: wht about label interdependency in BR approaches?<br>\n",
    "During the next weeks, we'll explore different paradigms in supervised learning. Your spectrum of choices will broaden and consequently your options: harder choices, greater benefits. <br>\n",
    "</div>\n",
    " \n",
    " Resources:\n",
    "  - Metrics: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "  - Model Validation:\n",
    "    - http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
    "    - http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "  - MLC architectures:\n",
    "    - https://en.wikipedia.org/wiki/Classifier_chains\n",
    "      - sklearn http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html#sklearn.multioutput.ClassifierChain\n",
    "      http://scikit-learn.org/stable/auto_examples/multioutput/plot_classifier_chain_yeast.html#sphx-glr-auto-examples-multioutput-plot-classifier-chain-yeast-py\n",
    "      - specialized MLC python package: http://scikit.ml/api/skmultilearn.problem_transform.cc.html#skmultilearn.problem_transform.ClassifierChain\n",
    "    - https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "  \n",
    "It is important, as a data scientist, to develop a strong intuition.  \n",
    "You have seen cases where model interpretation is most important (stats, linear regression); this will not be one of them.  \n",
    "SVM are mostly used as powerful prediction tools. The intuition will be rather in **when** to use it (big/small data, etc) and **how** to use it (tuning parameters, choosing kernels).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-challenge n. 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code10.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-challenge n.2: model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Just a few lines to get you started but you're on your own for this one!\n",
    "\n",
    "# === Hints: starting with a new dataset\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "\n",
    "# Load scene.arff via dedicate scipy.io function\n",
    "raw_data_2, metadata_2 = arff.loadarff('data/scene.arff')\n",
    "print(\"nrows:\", len(raw_data_2))    # 2417\n",
    "print(\"ncols:\", len(raw_data_2[0])) #  117\n",
    "\n",
    "df2 = pd.DataFrame(raw_data)\n",
    "print(\"dimensions:\", df2.shape)           # -> (2417, 117)\n",
    "print(df.head(5))         # for free, we get column names\n",
    "print(type(df.iloc[0,0])) # -> <class 'bytes'> ## we want to have plain {0,1} integers\n",
    "\n",
    "classes_list = [name for name in df2.columns if \"Class\" in name]\n",
    "print(classes_list)  # -> ['Class1', 'Class2', ... , 'Class14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

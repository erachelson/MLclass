{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bayesian models for Machine Learning<br>Gaussian Processes</div>\n",
    "\n",
    "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
    "$$\\mathbb{P}(Y=y|X=x)$$\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
    "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
    "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
    "</div>\n",
    "\n",
    "Note that Bayesian inference applies both to classification and regression.\n",
    "\n",
    "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
    "\n",
    "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
    "\n",
    "In this (second) notebook on Bayesian modeling in ML, we will explore Gaussian Processes.\n",
    "\n",
    "1. [A reminder on Gaussian distributions](#sec1)\n",
    "2. [Back to the regression problem](#sec2)\n",
    "3. [Gaussian Processes for classification](#sec3)\n",
    "4. [Examples](#sec4)\n",
    "    1. [Spam or ham?](#sec4-1)\n",
    "    2. [NIST](#sec4-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to learn a certain phenomenon $f(x)$, given some measurements $y = f(x)$ + noise.\n",
    "\n",
    "For example, that could be learning the pressure $P$ across the wingspan of a plane as a function of the position $l$ on the wing, the Mach value $M$, and the temperature $T$ (so $x=(l,M,T)$).\n",
    "\n",
    "Let's suppose that for a given value of $x$, the corresponding observation $y$ follows a Gaussian distribution of mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "Need a reminder on Gaussian distributions?\n",
    "\n",
    "# 1. <a id=\"sec1\"></a>A reminder on Gaussian distributions\n",
    "\n",
    "A Gaussian (or Normal) distributed variable has the following probability density function. The mean $\\mu$ is the most frequent value observed for this variable, while the variance $\\sigma^2$ indicates the diversity of values that $y$ often takes (how the distribution is spread-out along $y$).\n",
    "\n",
    "We say that $y$ is drawn from the Gaussian distribution of mean $\\mu$ and variance $\\sigma^2$ and write:\n",
    "$$y \\sim \\mathcal{N}\\left(\\mu,\\sigma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T07:29:14.405370Z",
     "start_time": "2018-10-23T07:29:13.736823Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size=(10, 10)\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 0.0\n",
    "sigma = 2.0\n",
    "x = np.linspace(-10., 10, 100)\n",
    "plt.plot(x, norm(mu,sigma).pdf(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take two values $x_1$ and $x_2$. The value taken by $y$ in $x_1$ is not independent of the value in $x_2$; we shall say that $y_1$ and $y_2$ are **correlated**. For example, the pressures $P$ in close $(l,M,T)$ values take values that are linked together.\n",
    "\n",
    "Let's still suppose the pair $(y_1,y_2)$ follows a Gaussian distribution in $x_1$ and $x_2$, with mean $(\\mu_1,\\mu_2)$. While the variance in the monovariate case was given by a single scalar $\\sigma^2$, now it is given by a covariance matrix $\\Sigma = \\left[\\begin{array}{cc} \\sigma_{11} & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma_{22} \\end{array}\\right]$ that depicts how the distribution spreads in the $(y_1,y_2)\\in\\mathbb{R}^2$ plane.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\y_2\\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu_1\\\\\\mu_2\\end{array}\\right], \\Sigma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T07:31:23.782076Z",
     "start_time": "2018-10-23T07:31:23.336230Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "x,y = np.mgrid[-1:1:.01, -1:1:.01]\n",
    "pos = np.dstack((x, y))\n",
    "mu = [0.0, 0.0]\n",
    "sigma = np.array([[2.0, 0.4], [0.4, 0.5]])\n",
    "plt.contourf(x, y, multivariate_normal(mu, sigma).pdf(pos));\n",
    "print(\"mu =\",mu)\n",
    "print(\"sigma:\")\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix indicates how the values of $y_1$ are centered around $\\mu_1$ (through $\\sigma_{11}$) and correlated with the value of $y_2$ (through $\\sigma_{12}$). In particular, $\\sigma_{12}$ indicates how similar the evolution of $y_1$ and $y_2$ are. Suppose we measure a sample $(\\hat{y}_1, \\hat{y}_2)$:\n",
    "- having $\\sigma_{12}>0$ indicates that if $\\hat{y}_1>\\mu_1$ then it is likely that $\\hat{y}_2>\\mu_2$ in a proportional fashion,\n",
    "- conversely $\\sigma_{12}<0$ indicates that $\\hat{y}_1$ and $\\hat{y}_2$ evolve in opposite directions relatively to their respective means,\n",
    "- $\\sigma_{12}=0$ means that $y_1$ and $y_2$ are independent: a sample of the former gives no information about how likely it is that the latter is greater or smaller than its mean.\n",
    "\n",
    "If there are $N$ such points $x_i$, and we assume again a Gaussian distribution of $y$, we end up with a mean $(\\mu_1,\\ldots,\\mu_N)$ and a covariance matrix $\\Sigma = \\left[\\begin{array}{ccc} \\sigma_{11} & \\ldots & \\sigma_{1N}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\sigma_{1N} & \\ldots & \\sigma_{NN} \\end{array}\\right]$.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu_1\\\\ \\vdots \\\\ \\mu_N\\end{array}\\right], \\Sigma\\right)$$\n",
    "\n",
    "Let's generalize to any number of points:\n",
    "- The mean becomes $\\mu(x)$\n",
    "- The covariance matrix can be obtained through a **covariance kernel** $k(x,x')$\n",
    "\n",
    "For a finite set of points $(x_1,\\ldots,x_N)$, the corresponding $(y_1,\\ldots,y_N)$ follow a Gaussian distribution of mean $(\\mu(x_1),\\ldots,\\mu(x_N))$ and of covariance matrix $\\Sigma = \\left[\\begin{array}{ccc} k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \\vdots & \\ddots & \\vdots \\\\ k(x_1,x_N) & \\ldots & k(x_N,x_N) \\end{array}\\right]$.\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\end{array}\\right] \\sim \\mathcal{N}\\left( \\left[\\begin{array}{c} \\mu(x_1)\\\\ \\vdots \\\\ \\mu(x_N)\\end{array}\\right], \\Sigma\\right)$$\n",
    "\n",
    "The information of $\\mu(x)$ and $k(x,x')$ defines a **Gaussian Process**. If we were able to learn a certain hidden function $f$ as a Gaussian Process, not only would we have the average function that fits $f$ best, but we would also have the probability that any other function fits $f$. In other words, in any new point $x$, we would have the average prediction $\\mu(x)$ but also an **uncertainty estimation** through the variance estimate $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a>2. Back to the regression problem\n",
    "\n",
    "So we assume that $y$ is drawn from a Gaussian Process.\n",
    "\n",
    "Recall that $\\mu(x)$ is the mean of the distribution in $x$ and that $k(x,x')$ describes how similar the values of $y$ and $y'$ (resp. in $x$ and $x'$) are likely to be spread around their respective means.\n",
    "\n",
    "To make things simple, assume that the Gaussian Process describing $f$ has mean zero, so $\\mu(x)=0$ everywhere. So what relates an observation to another is just the covariance function $k(x,x')$.\n",
    "\n",
    "A common covariance function is the so-called **squared exponential**:\n",
    "$$k(x,x') = \\sigma^2 e^{-\\frac{(x-x')^2}{2 l^2}}$$\n",
    "\n",
    "With this covariance function, the correlation between $y$ and $y'$ is high when $x$ and $x'$ are close. As $x$ and $x'$ get further from each other, $y$ and $y'$ tend to become independent.\n",
    "\n",
    "Since we assume that the observed data $\\left\\{(x_i,y_i)\\right\\}_{i=1...N}$ are drawn from a Gaussian distribution and that any new point $(x,y)$ should be drawn from that distribution too, we can write:\n",
    "\n",
    "$$\\left[\\begin{array}{c}y_1\\\\ \\vdots\\\\y_N \\\\y \\end{array}\\right] \\sim \\mathcal{N}\\left( 0_{N+1}, \\left[\\begin{array}{cccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N) & k(x_1, x)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots     & \\vdots \\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N) & k(x_N,x)\\\\\n",
    "k(x_1, x)  & \\ldots & k(x_N,x)   & k(x,x)\n",
    "\\end{array}\\right] \\right)$$\n",
    "\n",
    "(where $0_{N+1}$ is the $N+1$ dimensional zero vector)\n",
    "\n",
    "The line above simply states that all points, both the observed data points and the new points on which we wish to make a prediction, are drawn according to the same Gaussian Process. Let's simplify the writing using vector notation. Let's write:\n",
    "- the data points $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} = \\left[x_1,\\ldots,x_N\\right]$, \n",
    "- the data covariance matrix $K = \\left[\\begin{array}{ccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots\\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{array}\\right]$\n",
    "- The cross-covariance vector $K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$\n",
    "\n",
    "Then we have:\n",
    "$$\\left[\\begin{array}{c}\\mathbf{y}\\\\y \\end{array}\\right] \\sim \\mathcal{N}\\left( 0_{N+1}, \\left[\\begin{array}{cc}\n",
    "K & K_*(x)^T\\\\ \n",
    "K_*(x) & k(x,x)\n",
    "\\end{array}\\right] \\right)$$\n",
    "\n",
    "Let's take a step back. What we have written so far is the probability $\\mathbb{P}(\\mathbf{y},y|\\mathbf{x},x)$. But what we are interested in is the posterior distribution $\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x)$; that is \"the probability of the prediction $y$, given the data $(\\mathbf{x},\\mathbf{y})$\". So let's use Bayes theorem:\n",
    "\n",
    "$$\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x) = \\frac{\\mathbb{P}(\\mathbf{y},y|\\mathbf{x},x)}{\\mathbb{P}(\\mathbf{y}|\\mathbf{x},x)}$$\n",
    "\n",
    "It appears that because we have assumed that the values were drawn from a Gaussian Process, this posterior follows a Gaussian distribution too:\n",
    "$$\\mathbb{P}(y|\\mathbf{x},\\mathbf{y},x) = \\mathcal{N}\\left( K_*(x)K^{-1} \\mathbf{y}, k(x,x) - K_*(x) K^{-1} K_*(x)^T\\right)$$\n",
    "\n",
    "Consequently, our best estimate for $y$ is the mean of this distribution:\n",
    "$$\\bar{y} = K_*(x) K^{-1} \\mathbf{y}$$\n",
    "\n",
    "And the uncertainty in this estimate is captured by the variance:\n",
    "$$\\sigma(y)^2 = k(x,x)- K_*(x)K^{-1}K_*(x)^T$$\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "Given the input data $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} = \\left[x_1,\\ldots,x_N\\right]$, and given a covariance kernel $k(x,x')$, a Gaussian Process regressor estimates the distribution of $y(x)$ as a Gaussian $\\mathcal{N}(\\mu,\\sigma)$ with:\n",
    "<ul>\n",
    "<li> $\\mu = K_*(x)K^{-1} \\mathbf{y}$\n",
    "<li> $\\sigma(y)^2 = k(x,x) - K_*(x)K^{-1}K_*(x)^T$\n",
    "</ul>\n",
    "where:\n",
    "<ul>\n",
    "<li> $K = \\left[\\begin{array}{ccc}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N)\\\\ \n",
    "\\vdots     & \\ddots & \\vdots\\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{array}\\right]$\n",
    "<li>$K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's take a concrete example.\n",
    "\n",
    "Suppose the hidden function we want to learn is $f(x) = x \\sin(x)$.\n",
    "\n",
    "Suppose the observation is drawn as $y = f(x) + \\mathcal{N}(0, \\sigma_n^2)$.\n",
    "\n",
    "Let's draw 10 data samples and try to obtain the best fit function and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T07:51:13.988941Z",
     "start_time": "2018-10-23T07:51:13.376538Z"
    }
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x*np.sin(x)\n",
    "\n",
    "sigma_noise = 0.3\n",
    "def observation(x):\n",
    "    return func(x) + np.random.normal(0,sigma_noise,x.shape[0])\n",
    "\n",
    "X = np.linspace(-5,10,10)\n",
    "N = X.shape[0]\n",
    "Y = observation(X)\n",
    "x = np.linspace(-5,10,100)\n",
    "\n",
    "fig=plt.figure(figsize=(5,5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best function, we need a covariance kernel. We will take the square exponential kernel with arbitrary $\\sigma$ and $l$ parameters and will modify it slightly to account for the measurement noise $\\sigma_n$. Then we simply apply the formula above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:04:32.076937Z",
     "start_time": "2018-10-23T08:04:31.720492Z"
    }
   },
   "outputs": [],
   "source": [
    "sigma_kernel = 0.2\n",
    "l=0.3\n",
    "def kernel(x1,x2,sigma_kernel,l,sigma_noise):\n",
    "    return sigma_kernel * sigma_kernel * np.exp(-(x1-x2)**2 / (2*l*l)) + sigma_noise*(x1==x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice:**\n",
    "Your turn to play. Define two functions `compute_K(X,sigma_kernel,l,sigma_noise)` (that returns the covariance matrix) and `GPpredict(x_new, x_data, y_data, Kinv)` (that returns the mean and standard deviation of the output value) below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:04:32.076937Z",
     "start_time": "2018-10-23T08:04:31.720492Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "def compute_K(X,sigma_kernel,l,sigma_noise):\n",
    "    ### FILL THE VALUES OF K\n",
    "    N = X.shape[0]\n",
    "    K = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i,j] = kernel(X[i],X[j],sigma_kernel,l,sigma_noise)\n",
    "    return K\n",
    "\n",
    "def GPpredict(x_new, x_data, y_data, Kinv):\n",
    "    ### COMPUTE MEAN AND STANDARD DEVIATION\n",
    "    N = y_data.shape[0]\n",
    "    Kstar = np.zeros((1,N))\n",
    "    for i in range(N):\n",
    "        Kstar[0,i] = kernel(x_data[i], x_new, sigma_kernel, l, sigma_noise)\n",
    "    mu = Kstar @ Kinv @ y_data\n",
    "    sigma = kernel(x_new,x_new,sigma_kernel,l,sigma_noise) - Kstar @ Kinv @ Kstar.T\n",
    "    return mu, sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:04:32.076937Z",
     "start_time": "2018-10-23T08:04:31.720492Z"
    }
   },
   "outputs": [],
   "source": [
    "Kinv = np.linalg.inv(compute_K(X,sigma_kernel,l,sigma_noise))\n",
    "\n",
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = GPpredict(x[i], X, Y, Kinv)\n",
    "\n",
    "fig=plt.figure(figsize=(8,8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite disappointing. Indeed, the (blue) learned curve goes through the data points but it is far from the (red) original function. An interesting feature however is that the uncertainty reduces almost to zero around the data points and strongly increases elsewhere.\n",
    "\n",
    "What happened? It seems we made a poor arbitrary choice of kernel parameters. For instance taking $l=0.3$ implies that values $y$ and $y'$ for two points $x$ and $x'$ quickly tend to be independent and so the \"influence\" of a data point (a red point) does not extend far.\n",
    "\n",
    "Let's re-run this experiment with other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:07:38.502669Z",
     "start_time": "2018-10-23T08:07:38.163730Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma_kernel = 3\n",
    "l=1\n",
    "\n",
    "Kinv = np.linalg.inv(compute_K(X,sigma_kernel,l,sigma_noise))\n",
    "\n",
    "def GPpredict(x_new, x_data, y_data, Kinv):\n",
    "    N = y_data.shape[0]\n",
    "    Kstar = np.zeros((1,N))\n",
    "    for i in range(N):\n",
    "        Kstar[0,i] = kernel(x_data[i], x_new, sigma_kernel, l, sigma_noise)\n",
    "    mu = Kstar @ Kinv @ y_data\n",
    "    sigma = kernel(x_new,x_new,sigma_kernel,l,sigma_noise) - Kstar @ Kinv @ Kstar.T\n",
    "    return mu, sigma\n",
    "\n",
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = GPpredict(x[i], X, Y, Kinv)\n",
    "\n",
    "fig=plt.figure(figsize=(8,8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starts to look better. The blue curve fits the data points better but it's still not perfect and the uncertainty between data points is actually quite large.\n",
    "\n",
    "We could wish for an automated way of tuning these kernel parameters. Let's call them $\\theta$.\n",
    "\n",
    "One way to do that is to search for the value of $\\theta$ that maximizes the probability of the observed $\\mathbf{y}$ given the inputs $\\mathbf{x}$. In other words, to find the best $\\theta$, we search for the one that explains best the data that we already know, and then use it to make predictions on new points. That is called **Marginal Likelihood Maximization**.\n",
    "\n",
    "Fortunately for use, scikit-learn includes all these operations in a single procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:09:34.601535Z",
     "start_time": "2018-10-23T08:09:34.403799Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gp = GaussianProcessRegressor()\n",
    "X=X.reshape(-1,1)\n",
    "Y=Y.reshape(-1,1)\n",
    "gp.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:09:38.849585Z",
     "start_time": "2018-10-23T08:09:38.541507Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5,10,100)\n",
    "y = np.zeros(x.shape)\n",
    "sigma = np.zeros(x.shape)\n",
    "for i in range(x.shape[0]):\n",
    "    y[i], sigma[i] = gp.predict(x[i], return_std=True)\n",
    "\n",
    "fig=plt.figure(figsize=(8,8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, func(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, Y, 'r.', markersize=10, label=u'Observations')\n",
    "plt.plot(x, y, 'b-', label=u'Prediction')\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y - 1.9600 * sigma,\n",
    "                        (y + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-6, 10)\n",
    "plt.legend(loc='upper left');\n",
    "print(gp.kernel_.hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's better. Although the correct function was not found exactly, given the few data points we had it's already a good fit.\n",
    "\n",
    "Let's summarize a few properties of Gaussian Processes.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> Gaussian processes are an offline method\n",
    "<li> They are an optimal method (in the sense of Bayesian inference)\n",
    "<li> They provide a best fit function, but also error bounds\n",
    "<li> Given $N$ data points, they require the inversion of an $N\\times N$ covariance matrix (complexity in $O(N^3)$)\n",
    "<li> The kernel encodes the prior knowledge about the function's behaviour.\n",
    "<li> Careful choice / design of kernels can make GPs a very powerful tool... but can also make them very unstable computationally.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Let's illustrate this last property on a final example, with a periodic function and the \"Exp-Sine-Squared\" kernel that is commonly used for periodic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:18:55.241050Z",
     "start_time": "2018-10-23T08:18:54.699618Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "\n",
    "# Generate sample data\n",
    "rng = np.random.RandomState(0)\n",
    "X = 15 * rng.rand(100, 1)\n",
    "y = np.sin(X).ravel()\n",
    "y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n",
    "\n",
    "# GP training\n",
    "gp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + WhiteKernel(1e-1)\n",
    "gpr = GaussianProcessRegressor(kernel=gp_kernel)\n",
    "#gpr = GaussianProcessRegressor()\n",
    "gpr.fit(X, y)\n",
    "\n",
    "# GP prediction\n",
    "X_plot = np.linspace(0, 25, 10000)[:, None]\n",
    "y_gpr, y_std = gpr.predict(X_plot, return_std=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=fig_size)\n",
    "lw = 2\n",
    "plt.scatter(X, y, c='k', label='data')\n",
    "plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')\n",
    "plt.plot(X_plot, y_gpr, color='darkorange', lw=lw,\n",
    "         label='GPR (%s)' % gpr.kernel_)\n",
    "plt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',\n",
    "                 alpha=0.2)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(-4, 4)\n",
    "plt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a>3. Gaussian Processes for classification ?\n",
    "\n",
    "It actually works exactly the same way. For binary classification problems, Gaussian Processes try to predict the probability of belonging to the first class (thus turning back to a regression problem).\n",
    "\n",
    "More precisely, they try to build a hidden function $f(x)$ so that: \n",
    "$$\\mathbb{P}(Y=0|X=x) = \\frac{1}{1+e^{f(x)}}$$\n",
    "Where $f$ is a Gaussian Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> 4. Examples\n",
    "\n",
    "## <a id=\"sec4-1\"></a> 4.1 Spam or ham?\n",
    "\n",
    "Let's illustrate that on the Ling-spam database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:22:02.068822Z",
     "start_time": "2018-10-23T08:21:57.103287Z"
    }
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded a training set with 2000 examples. This means that the Gaussian Process training will require inverting a $2000\\times2000$ matrix. Although this is still feasible in a few seconds or minutes, we are touching here a limitation of Gaussian Processes in their ability to scale up to large datasets.\n",
    "\n",
    "<div class=\"alert alert-warning\">**Exercice:**<br>\n",
    "Use scikit-learn to fit a [Gaussian Process classifier](http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc) to the spam data loaded above. Estimate the generalization score.<br>\n",
    "Hint: you can use the `.toarray()` method of sparse matrices to get the corresponding dense matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:33:53.694630Z",
     "start_time": "2018-10-23T08:32:57.423489Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fair comparison, we can also try the training on the word count features rather than Tf-Idf ones. Although it would probably be more beneficial to search for a better kernel than the default squared exponential one.\n",
    "\n",
    "<div class=\"alert alert-warning\">**Exercice:**<br>\n",
    "Use scikit-learn to fit a Gaussian Process classifier to the raw word count data loaded below. Estimate the generalization score.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:34:08.265221Z",
     "start_time": "2018-10-23T08:34:08.259067Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:35:17.341764Z",
     "start_time": "2018-10-23T08:34:15.431177Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/code7.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep this last classifier and identify which are the misclassified emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_GP = GaussianProcessClassifier()\n",
    "spam_GP.fit(Xtrain.toarray(),ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:36:25.583975Z",
     "start_time": "2018-10-23T08:36:09.178688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_GP.predict(Xtest.toarray())\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:36:25.591676Z",
     "start_time": "2018-10-23T08:36:25.585685Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[0]+2000\n",
    "print(\"Prediction:\", spam_GP.predict(spam_data.word_count[index,:].toarray()))\n",
    "spam_data.print_email(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec4-2\"></a> 4.2 NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:36:58.938911Z",
     "start_time": "2018-10-23T08:36:58.854669Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:37:21.844184Z",
     "start_time": "2018-10-23T08:37:21.835911Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T08:37:33.987664Z",
     "start_time": "2018-10-23T08:37:26.782859Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_gp = GaussianProcessClassifier()\n",
    "digits_gp.fit(Xtrain,ytrain)\n",
    "prediction = digits_gp.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_gp.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect illustration that (often) ML methods don't work out-of-the-box and require some engineering knowledge just like any other modeling method. Here for example, a data scientist might investigate the kernel's properties, the marginal likelihood optimization procedure, the data distribution..., while the computer vision expert might question the use of pixel data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
